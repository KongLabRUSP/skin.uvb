---
title: "kin UVB SKH1 mouse model treated with SFN "
output:
  html_notebook: default
  html_document:
    df_print: paged
  pdf_document: default
---

# Part 1: RNA
```{r header, warning = FALSE, echo = FALSE, message = FALSE}
require(data.table)
require(DESeq2)
require(readxl)
require(BiocParallel)
require(ggplot2)
require(knitr)
# NOTE on DESeq2 Output: 'baseMean' is the average of the normalized count values, 
# divided by the size factors, taken over all samples in the DESeqDataSet
```

## Load RNA samples
Out of 30 samples, we selected 18 for this study. These are the normal tissue samples form the control, the UVA and the UVA+SFN treatment groups. normal tissue samples from the UVB_UA groups as well as tumor samples were excluded from this analysis.     
First, we removed 7,148 genes with zero counts in > 80% (> 14 out of 18) of samples. 17,273 out of 24,421 genes left. 
         
```{r data_rna, warning = FALSE, echo = FALSE, message = FALSE}
# Load data----
dt0 <- fread("data/renyi_dedup_rnaseq_data/featurescounts_uvb-skin_dedup_renyi_2-9-2018.csv",
             skip = 1)

# Remove unused columns----
dt1 <- dt0[, c(1, 6:ncol(dt0)), with = FALSE]

cnames <- colnames(dt1)[-c(1:2)]
cnames <- gsub(x = cnames,
               pattern = ".dedup.bam",
               replacement = "")
colnames(dt1)[-c(1:2)] <- cnames

# ATTENTION! In this analysis, we will only examine controls and SFN
# Also, removed cancer cell samples
tnames <- substr(x = colnames(dt1), 
                 start = 3,
                 stop = 3)

gnames <- substr(x = colnames(dt1), 
                 start = 5,
                 stop = 7)

dt1 <- dt1[, gnames %in% c("id",
                           "th",
                           "CON",
                           "UVB",
                           "SFN" ) &
             tnames != "t",
           with = FALSE]
# 18 samples left

# Remove genes with zero counts in > 80% (> 14 out of 18) of samples
tmp <- dt1[, -c(1:2)] == 0
tmp <- rowSums(tmp) > 14
sum(tmp)

dt1 <- droplevels(dt1[!tmp, ])
nrow(dt1)
# 17,273 out of 24,421 genes left

DT::datatable(head(dt1, 10),
              rownames = FALSE,
              options = list(pageLength = 10),
              caption = "Table 1: first 10 rows of the count table")
```

## Transcripts per kilobase million (TPM) normalization
Next, we noramized the counts. To convert number of hits to  the relative abundane of genes in each sample, we used ***transcripts per kilobase million (TPM)*** normalization, which is as following for the j-th sample:       
1. normilize for gene length: a[i, j] = 1,000*count[i, j]/gene[i, j] length(bp)     
2. normalize for seq depth (i.e. total count): a(i, j)/sum(a[, j])     
3. multiply by one million     
A very good comparison of normalization techniques can be found at the following video:    
[RPKM, FPKM and TPM, clearly explained](https://www.rna-seqblog.com/rpkm-fpkm-and-tpm-clearly-explained/)
     
After the normalization, each sample's total is 1M:
     
```{r tpm, warning = FALSE, echo = FALSE, message = FALSE}
# Normalize counts to TPM
tmp <- 1000*dt1[, 3:ncol(dt1)]/dt1$Length
tpm <- data.table(gene = dt1$gene,
                  apply(tmp,
                        2,
                        function(a) {
                          10^6*(a/sum(a))
                        }))
colSums(tpm[, -c(1:2)])
DT::datatable(head(tpm, 10),
              rownames = FALSE,
              options = list(pageLength = 10,
                             digits = 2),
              caption = "Table 2: transcripts per kilobase million (TPM) normalized counts")
```

# Session Information
```{r info,eval=TRUE}
sessionInfo()
```